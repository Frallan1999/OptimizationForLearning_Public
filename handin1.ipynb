{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"22\"\n",
    "NAME1 = \"Ivar FagerfjÃ¤ll\"\n",
    "NAME2 = \"Hanna Frederiksen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "# Optimization for learning - FRTN50\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "The goal of this assignment is to become familiar with some of the steps involved in solving an optimization problem. In this assignment, you will form Fenchel dual problems, find gradients and/or proximal operators, and implement the proximal gradient method.\n",
    "\n",
    "__Problem__ The problem we will solve is the following constrained problem\n",
    "\n",
    "\\begin{align}\\label{eq:the_problem}\\tag{1}\n",
    "\t\\underset{x \\in S}{\\text{minimize}}\\; \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align}\n",
    "\n",
    "where $Q\\in\\mathbb{S}_{++}^{n}$, $q\\in\\mathbb{R}^{n}$ and $S\\subseteq\\mathbb{R}^{n}$ is a set defined by the points $a,b\\in\\mathbb{R}^{n}$, $a\\leq b$, such that \n",
    "\n",
    "\\begin{align*}\n",
    "\tS = \\{x \\in \\mathbb{R}^{n}: a \\leq x \\leq b \\}.\n",
    "\\end{align*}\n",
    "\n",
    "I.e., we are going to minimize a quadratic function over an $n$-dimensional box. Recall that, the vector inequality $a\\leq b$ means that \n",
    "\n",
    "\\begin{align*}\n",
    "\ta_{i} \\leq b_{i}\n",
    "\\end{align*}\n",
    "\n",
    "for each $i=1,\\ldots,n$. Define the function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "\tf(x) = \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align*}\n",
    "\n",
    "for each $x\\in\\mathbb{R}^{n}$ and let $\\iota_{S}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ denote the indicator function of the set $S$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}(x) =\n",
    "\t\\begin{cases}\n",
    "\t\t0 \t\t& \\text{if }x\\in S, \\\\\n",
    "\t\t\\infty \t& \\text{if }x\\in \\mathbb{R}^n \\setminus S.\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Problem \\eqref{eq:the_problem} can then be written as \n",
    "\n",
    "\\begin{align}\\label{eq:the_problem_mod}\\tag{2}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + \\iota_{S}(x).\n",
    "\\end{align}\n",
    "\n",
    "__Solution method__ To solve optimization problem \\eqref{eq:the_problem_mod}, we will use the _proximal gradient method_. It solves problems of the form\n",
    "\n",
    "\\begin{align}\\label{eq:pgprob}\\tag{3}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + g(x) \n",
    "\\end{align}\n",
    "\n",
    "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is differentiable and $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is proximable, i.e., $\\prox_{\\gamma g}$ can be cheaply computed. The proximal gradient method (with constant step-size) is given by:\n",
    "\n",
    "- Pick some arbitrary initial guess $x^0\\in\\R^{n}$ and step-size $\\gamma>0$.\n",
    "- For $k=0,1,2\\ldots$, let \n",
    "\\begin{align}\\label{eq:pg}\\tag{4}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma g}\\left(x^k - \\gamma \\nabla f(x^k)\\right).\n",
    "\\end{align}\n",
    "- Stop when $x^k$ is deemed to have converged.\n",
    "\n",
    "In this assignment, we simply run the proximal gradient method a large fixed number of iterations and plot the norm of the residual/step-length, $\\norm{x^{k+1} - x^k}_{2}$, of each step to make sure it converges to zero. Since the experiments are run on a computer, zero means smaller than machine precision, which usually is around $10^{-15}$.\n",
    "\n",
    "The step-size parameter $\\gamma$ in the \\eqref{eq:pg} will affect the convergence. It should be tuned to the problem or chosen based on properties of $f$ and $g$. In particular, suppose that $f$ and $g$ are proper, closed and convex. \n",
    "If $f$ is $\\beta$-smooth for some parameter $\\beta>0$, the maximal step-size to guarantee convergence is $\\gamma < \\frac{2}{\\beta}$.\n",
    "\n",
    "Below are the tasks that you need to solve. Keep this in mind:\n",
    "- The suggested exercises in the exercise compendium found on the Canvas course page, up until and including the chapter \"Proximal gradient method - basics\", is relevant for this assignment. \n",
    "- Carefully motivate every step in your calculations.\n",
    "- Use figures and tables to motivate your answers.\n",
    "- Figures must have appropriately labeled axes and must be referenced in the main text.\n",
    "- Your code should be written in a quite general manner, i.e., if a question is slightly modified, it should only require slight modifications in your code as well. \n",
    "- Comment your code well. \n",
    "- Make sure you plot in such a way that small quantities (e.g., $\\norm{x^{k+1} - x^k}_{2}$) are visible. In particular, use log-linear plots, where the quantity that should go to $0$ is on the $y$-axis using logarithmic scale, and the iteration number $k$ on the $x$-axis using linear scale.\n",
    "- What you need to submit to Canvas:\n",
    "    - This jupyter notebook containing your solutions.\n",
    "    - An exported pdf version of the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 1:\n",
    "\n",
    "Show that $f$ and $\\iota_{S}$ in (2) are convex and show that constraint qualification (CQ) holds. You are allowed to assume that $\\relint S \\neq \\emptyset$. Note that $f$ and $\\iota_{S}$ also are closed, but you do not need to prove this.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "The second order condition for convexity says that assuming a twice continuous differentiable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ this is convex if and only if \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\nabla^2f(x) \\succcurlyeq 0 \n",
    "\\end{align*}\n",
    "\n",
    "for all $x\\in\\mathbb{R}^{n}$. I.e that it is positive semidefinite. \n",
    "\n",
    "The function f is defined as \n",
    "\n",
    "\\begin{align*}\n",
    "\tf(x) = \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align*}\n",
    "\n",
    "and has the Hessian matrix  \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\nabla^2f(x) = Q\n",
    "\\end{align*}\n",
    "\n",
    "where it is given that $Q\\in\\mathbb{S}_{++}^{n}$. This set ${S}_{++}^{n}$ denotes that\n",
    "\n",
    "    \n",
    "\\begin{align*}\n",
    "\t{S}_{++}^{n} = \\{A \\in \\mathbb{S}^{n} | A \\prec 0 \\}\n",
    "\\end{align*}\n",
    "\n",
    "Since positive definit is a stronger notation than positive semi definit, f is convex by the second order condition for convexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 2:\n",
    "\n",
    "Compute the conjugate functions $f^\\ast$ and $\\iota_{S}^\\ast$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "The conjugate function of $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is defined as \n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(x) := \\sup_{x}(s^Tx-f(x)).\n",
    "\\end{align*}\n",
    "\n",
    "Starting with coputation of $f^\\ast(s)$ this gives us\n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(s) = \\sup_{x}(s^Tx-\\tfrac{1}{2}x^T Q x - q^Tx).\n",
    "\\end{align*}\n",
    "\n",
    "I.e the conjugate is found by the largest value of this optimization problem. Since Q is positive definite the function is strongly concave, and the maximum therefore exists and is unique. Since the function also is differentiable, Fermat's rule says that $x$ is found by setting the gradient to zero. That is, \n",
    "\n",
    "\\begin{align*}\n",
    "\t0 = s - Qx - q.    \n",
    "\\end{align*}\n",
    "\n",
    "which can be rewritten as \n",
    "\n",
    "\\begin{align*}\n",
    "\tx = Q^{-1}(s-q).    \n",
    "\\end{align*}\n",
    "\n",
    "By inserting the maximizing argument we obtain the largest value, i.e the conjugate. This gives: \n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(s) = s^TQ^{-1}(s-q) - \\tfrac{1}{2}{(s-q)}^TQ^{-1} Q Q^{-1}(s-q) - q^TQ^{-1}(s-q) \n",
    "    = \\tfrac{1}{2}(s-q)^TQ^{-1}(s-q)\n",
    "\\end{align*}\n",
    "\n",
    "--- \n",
    "\n",
    "Now for computating $\\iota_{S}^\\ast$ we denote \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}^\\ast(s) = \\sup_{x}(s^Tx-\\iota_{S}(x)).\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\iota_{S}(x)$ is $\\infty$ for $x\\in \\mathbb{R}^n \\setminus S$ and $0$ for $x\\in \\mathbb{S}$, and it is $-\\iota_{S}(x)$ in the optimization problem, the maximum value will be found for $x\\in \\mathbb{S}$. The conjugate can therefore be rewritten as\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}^\\ast(s) = \\sup_{x\\in \\mathbb{S}}(s^Tx).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We now consider two different cases for $s$, and start with $s < 0$. For $s < 0$ every $s_{i} < 0$. From the set S we have $S = \\{x \\in \\mathbb{R}^{n}: a \\leq x \\leq b \\}$. Moreover, for both negative and positive a,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}} \\sum_{i=1}^{n} s_i x_i \\leq s^Ta,\n",
    "\\end{align*}\n",
    "\n",
    "and by the definition\n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sup_{x\\in \\mathbb{S}}(s^Tx) \\geq s^Ta.\n",
    "\\end{align*}\n",
    " \n",
    "Therefor \n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = s^Ta \n",
    "\\end{align*}\n",
    "\n",
    "for this case. \n",
    "\n",
    "In the other case, where $s < 0$ does not hold, $s_{j} \\geq 0$.\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}} \\sum_{i=1}^{n} s_i x_i \\leq s^Tb,\n",
    "\\end{align*}\n",
    "\n",
    "and by the definition\n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sup_{x\\in \\mathbb{S}}(s^Tx) \\geq s^Tb.\n",
    "\\end{align*}\n",
    "\n",
    "Therefor \n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = s^Tb \n",
    "\\end{align*}\n",
    "\n",
    "in this case.\n",
    "\n",
    "We have now covered all cases and conclude that \n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\max(s^Ta, s^Tb)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "----- \n",
    "\n",
    "Let $k\\in \\mathbb \\{1,\\ldots,n\\}$ be any index such that \n",
    "\n",
    "\\begin{align*}\n",
    "    s_{k} = \\min_{i = \\{1,\\ldots,n\\}}s_i\n",
    "\\end{align*}\n",
    "\n",
    "and let $j\\in \\mathbb \\{1,\\ldots,n\\}$ be any index such that \n",
    "\n",
    "\\begin{align*}\n",
    "    s_{j} = \\max_{i = \\{1,\\ldots,n\\}}s_i\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We have now covered all cases and conclude that \n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\max(a\\min_{i = \\{1,\\ldots,n\\}}s_i, b\\max_{i = \\{1,\\ldots,n\\}}s_i)\n",
    "\\end{align*}\n",
    "\n",
    "If $a \\geq 0$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}} \\sum_{i=1}^{n} s_i x_i \n",
    "    = \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}}(s_jx_j + \\sum_{i=1, i\\ne j}^{n} s_i x_i)\n",
    "    \\\\ \\leq \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}}(s_jx_j + \\sum_{i=1, i\\ne j}^{n} s_j x_i)\n",
    "    = \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}}s_j\\sum_{i=1, i\\ne j}^{n} x_i\n",
    "     \\leq s_j b\n",
    "\\end{align*}\n",
    "\n",
    "and by the definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 3:\n",
    "\n",
    "Write down a Fenchel dual problem to (2). Show that constraint qualification for the dual problem (CQ-D) holds.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 4:\n",
    "\n",
    "Show that $f$ and $f^*$ are $\\beta$-, and $\\beta^*$-smooth, respectively. Find expressions for the smallest such parameters $\\beta$ and $\\beta^*$.\n",
    "\n",
    "_Hint:_ Later when calculating the smoothness parameters in Pyhton, make sure to read the documentation carefully so that you use the correct function.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "As previously shown $f$ and $f^*$ are convex. The second-order condition for smoothness for convex functions states that:\n",
    "\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f(x) \\preceq \\beta I\n",
    "\\end{align*}\n",
    "\n",
    "For $f$ and $f^*$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f(x) = Qx + q \\implies \\nabla^2 f(x) = Q\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f^* (x) = Q^{-1} (s-q) \\implies \\nabla^2f^* (x) = Q^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "This gives:\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f(x) = Q \\preceq \\beta I\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f^* (x) = Q^{-1} \\preceq \\beta^* I\n",
    "\\end{align*}\n",
    "\n",
    "The smallest parameters for \\beta and \\beta^* are therefor:\n",
    "\n",
    "\\begin{align*} \n",
    "    \\beta = \\lambda_{max}\n",
    "\\end{align*}\n",
    "\\begin{align*} \n",
    "    \\beta^* = \\lambda^*_{max}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_{max}$ is the biggest eigenvalue for $Q$ and $\\lambda^*_{max}$ is the biggest eigenvalue for $Q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$\n",
    "\n",
    "---\n",
    "### Task 5:\n",
    "\n",
    "Compute $\\nabla f$, $\\nabla f^\\ast$, $\\prox_{\\gamma\\iota_{S}}$ and $\\prox_{\\gamma\\iota_{S}^\\ast}$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "\n",
    "__Solution:__ \n",
    "From previous tasks we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f(x) = Qx + q\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f^* (x) = Q^{-1} (s-q)\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 6:\n",
    "\n",
    "Based on your results above, write explicitly out the proximal gradient update rule (4) for both the primal and the dual problem. Use $x$ as the primal variable and $\\mu$ as the dual variable.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 7:\n",
    "\n",
    "Suppose that $\\mu^\\star\\in\\R^{n}$ is an optimal solution to the dual problem you found in Task 3. Given $\\mu^\\star$, and starting from the optimality condition for the dual problem (given by Fermat's rule), recover an optimal point $x^{\\star}\\in\\R^{n}$ to the primal problem (2), and show that this $x^{\\star}$ is in fact an optimal solution to the primal problem (2).\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "--- \n",
    "### Task 8:\n",
    "\n",
    "Use your results above to fill in the functions below.\n",
    "\n",
    "__Solution:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    quad(x,Q,q) computes the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    quadconj(mu,Q,q) computes the conjugate function of the \n",
    "    quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def box(x,a,b):\n",
    "    \"\"\"\n",
    "    box(x,a,b) computes the indicator function of the box contraint\n",
    "    [a,b]\n",
    "    \n",
    "    :param x: the variable of the indicator function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: 0 if x is in [a,b] and infinity otherwise\n",
    "    \"\"\"\n",
    "    if np.all(a <= x) and np.all(x <= b):\n",
    "        return 0\n",
    "    else: \n",
    "        return np.Inf\n",
    "\n",
    "def boxconj(mu,a,b):\n",
    "    \"\"\"\n",
    "    boxconj(mu,a,b) computes the conjugate function of the indicator function \n",
    "    of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: conjugate of the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return \n",
    "\n",
    "def grad_quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quad(x,Q,q) computes the gradient of the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def grad_quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quadconj(mu,Q,q) computes the gradient of the conjugate function of the \n",
    "    the quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of the conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def prox_box(x,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(x,a,b,gamma) computes proximal operator of the indicator function \n",
    "    of the box contraint [a,b], evaluated at x\n",
    "    \n",
    "    :param x: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the indicator function of the \n",
    "    box contraint [a,b], evaluated at x\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def prox_boxconj(mu,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(mu,a,b,gamma) computes proximal operator of the conjugate function of \n",
    "    the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the conjugate function of the indicator function of the \n",
    "    box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return\n",
    "\n",
    "def dual_to_primal(mu,Q,q,a,b):\n",
    "    \"\"\"\n",
    "    dual_to_primal(mu,Q,q,a,b) computes the solution x* to the primal problem \n",
    "    given a solution mu* to the dual problem.\n",
    "    \n",
    "    :param mu: the dual variable\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: the extracted primal variable\n",
    "    \"\"\"\n",
    "    # Write your solution here\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 9:\n",
    "\n",
    "Below is a function for generating $Q$, $q$, $a$, and $b$ that define the quadratic function $f$ and the box constraint set $S$. Use Task 8 to solve the primal problem using the proximal gradient method.\n",
    "\n",
    "__a)__ What seems to be the best choice of $\\gamma$? \n",
    "\n",
    "__b)__ Does the upper bound $\\gamma < \\frac{2}{\\beta}$ seem reasonable?\n",
    "\n",
    "\n",
    "Test different initial points for the algorithm:\n",
    "\n",
    "__c)__ Does this affect the point the algorithm converges to? \n",
    "\n",
    "__d)__ Reason about why/why not it affects the final point. _Hint:_ Look at the objective function in (2).\n",
    "\n",
    "__e)__ Does your final point $x^{\\text{final}}$ satisfy the constraint $x^{\\text{final}} \\in S$?\n",
    "\n",
    "__f)__ What about the iterates, do they always satisfy the constraint, $x^k \\in S$? Why/why not?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_data():\n",
    "    \"\"\"\n",
    "    problem_data() generates the problem data variables Q, q, a and b\n",
    "    \n",
    "    :return: (Q,q,a,b)\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1)))\n",
    "    n = 20\n",
    "    Q = rs.randn(n,n)\n",
    "    Q = Q.T@Q\n",
    "    q = rs.randn(n)\n",
    "    a = -rs.rand(n)\n",
    "    b = rs.rand(n)\n",
    "    return Q, q, a, b\n",
    "\n",
    "(Q,q,a,b) = problem_data()\n",
    "\n",
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 10:\n",
    "\n",
    "Solve the dual problem. \n",
    "\n",
    "__a)__ Similar to the previous task, find/verify the upper bound on the step-size and find a good step-size choice.\n",
    "\n",
    "Let $x^{\\text{final}}$ be the final points from Task 9 and $\\mu^{\\text{final}}$ the final point for the dual problem. Let $\\hat{x}^{\\text{final}}$ final primal points extracted from the final dual point $\\mu^{\\text{final}}$ using the expression from Task 7:\n",
    "\n",
    "__b)__ Are $x^{\\text{final}}$ and $\\hat{x}^{\\text{final}}$ the same?\n",
    "\n",
    "__c)__ Is $\\hat{x}^{\\text{final}}$ in the box $S$?\n",
    "\n",
    "__d)__ Let $\\mu^k$ be the iterates of the dual method, using the expression from Task 7, extract the primal iterates $\\hat{x}^k$ from $\\mu^k$. Does $\\hat{x}^k$ always satisfy the constraint $\\hat{x}^k \\in S$?\n",
    "\n",
    "Also: \n",
    "\n",
    "__e)__ How do the function values $f\\left(\\hat{x}^k\\right)$ develop over the iterations?\n",
    "\n",
    "__f)__ What about $f\\left(\\hat{x}^k\\right)+\\iota_{S}\\left(\\hat{x}^k\\right)$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
