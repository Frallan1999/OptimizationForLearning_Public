{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"22\"\n",
    "NAME1 = \"Ivar Fagerfjäll\"\n",
    "NAME2 = \"Hanna Frederiksen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "# Optimization for learning - FRTN50\n",
    "\n",
    "## Assignment 1\n",
    "\n",
    "The goal of this assignment is to become familiar with some of the steps involved in solving an optimization problem. In this assignment, you will form Fenchel dual problems, find gradients and/or proximal operators, and implement the proximal gradient method.\n",
    "\n",
    "__Problem__ The problem we will solve is the following constrained problem\n",
    "\n",
    "\\begin{align}\\label{eq:the_problem}\\tag{1}\n",
    "\t\\underset{x \\in S}{\\text{minimize}}\\; \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align}\n",
    "\n",
    "where $Q\\in\\mathbb{S}_{++}^{n}$, $q\\in\\mathbb{R}^{n}$ and $S\\subseteq\\mathbb{R}^{n}$ is a set defined by the points $a,b\\in\\mathbb{R}^{n}$, $a\\leq b$, such that \n",
    "\n",
    "\\begin{align*}\n",
    "\tS = \\{x \\in \\mathbb{R}^{n}: a \\leq x \\leq b \\}.\n",
    "\\end{align*}\n",
    "\n",
    "I.e., we are going to minimize a quadratic function over an $n$-dimensional box. Recall that, the vector inequality $a\\leq b$ means that \n",
    "\n",
    "\\begin{align*}\n",
    "\ta_{i} \\leq b_{i}\n",
    "\\end{align*}\n",
    "\n",
    "for each $i=1,\\ldots,n$. Define the function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ such that\n",
    "\n",
    "\\begin{align*}\n",
    "\tf(x) = \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align*}\n",
    "\n",
    "for each $x\\in\\mathbb{R}^{n}$ and let $\\iota_{S}:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ denote the indicator function of the set $S$, i.e.,\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}(x) =\n",
    "\t\\begin{cases}\n",
    "\t\t0 \t\t& \\text{if }x\\in S, \\\\\n",
    "\t\t\\infty \t& \\text{if }x\\in \\mathbb{R}^n \\setminus S.\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Problem \\eqref{eq:the_problem} can then be written as \n",
    "\n",
    "\\begin{align}\\label{eq:the_problem_mod}\\tag{2}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + \\iota_{S}(x).\n",
    "\\end{align}\n",
    "\n",
    "__Solution method__ To solve optimization problem \\eqref{eq:the_problem_mod}, we will use the _proximal gradient method_. It solves problems of the form\n",
    "\n",
    "\\begin{align}\\label{eq:pgprob}\\tag{3}\n",
    "\t\\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(x) + g(x) \n",
    "\\end{align}\n",
    "\n",
    "where $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is differentiable and $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is proximable, i.e., $\\prox_{\\gamma g}$ can be cheaply computed. The proximal gradient method (with constant step-size) is given by:\n",
    "\n",
    "- Pick some arbitrary initial guess $x^0\\in\\R^{n}$ and step-size $\\gamma>0$.\n",
    "- For $k=0,1,2\\ldots$, let \n",
    "\\begin{align}\\label{eq:pg}\\tag{4}\n",
    "\t\t\t\tx^{k+1} = \\prox_{\\gamma g}\\left(x^k - \\gamma \\nabla f(x^k)\\right).\n",
    "\\end{align}\n",
    "- Stop when $x^k$ is deemed to have converged.\n",
    "\n",
    "In this assignment, we simply run the proximal gradient method a large fixed number of iterations and plot the norm of the residual/step-length, $\\norm{x^{k+1} - x^k}_{2}$, of each step to make sure it converges to zero. Since the experiments are run on a computer, zero means smaller than machine precision, which usually is around $10^{-15}$.\n",
    "\n",
    "The step-size parameter $\\gamma$ in the \\eqref{eq:pg} will affect the convergence. It should be tuned to the problem or chosen based on properties of $f$ and $g$. In particular, suppose that $f$ and $g$ are proper, closed and convex. \n",
    "If $f$ is $\\beta$-smooth for some parameter $\\beta>0$, the maximal step-size to guarantee convergence is $\\gamma < \\frac{2}{\\beta}$.\n",
    "\n",
    "Below are the tasks that you need to solve. Keep this in mind:\n",
    "- The suggested exercises in the exercise compendium found on the Canvas course page, up until and including the chapter \"Proximal gradient method - basics\", is relevant for this assignment. \n",
    "- Carefully motivate every step in your calculations.\n",
    "- Use figures and tables to motivate your answers.\n",
    "- Figures must have appropriately labeled axes and must be referenced in the main text.\n",
    "- Your code should be written in a quite general manner, i.e., if a question is slightly modified, it should only require slight modifications in your code as well. \n",
    "- Comment your code well. \n",
    "- Make sure you plot in such a way that small quantities (e.g., $\\norm{x^{k+1} - x^k}_{2}$) are visible. In particular, use log-linear plots, where the quantity that should go to $0$ is on the $y$-axis using logarithmic scale, and the iteration number $k$ on the $x$-axis using linear scale.\n",
    "- What you need to submit to Canvas:\n",
    "    - This jupyter notebook containing your solutions.\n",
    "    - An exported pdf version of the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 1:\n",
    "\n",
    "Show that $f$ and $\\iota_{S}$ in (2) are convex and show that constraint qualification (CQ) holds. You are allowed to assume that $\\relint S \\neq \\emptyset$. Note that $f$ and $\\iota_{S}$ also are closed, but you do not need to prove this.\n",
    "\n",
    "__Solution:__\n",
    "\n",
    "Let $x,y \\in S = \\{x \\in \\R^n : a \\leq x \\leq b\\}$ and $\\theta \\in [0,1]$. We can then conclude that:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\theta x + (1-\\theta)y \\leq \\theta b + (1-\\theta)b = b \\\\\n",
    "    \\theta x + (1-\\theta)y \\geq \\theta a + (1-\\theta)a = a\n",
    "\\end{align*}\n",
    "\n",
    "Which implies $\\theta x + (1-\\theta)y \\in S$ and S is therefor convex.\n",
    "\n",
    "---\n",
    "\n",
    "The second order condition for convexity says that assuming a twice continuous differentiable function $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ this is convex if and only if \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\nabla^2f(x) \\succcurlyeq 0 \n",
    "\\end{align*}\n",
    "\n",
    "for all $x\\in\\mathbb{R}^{n}$. I.e that it is positive semidefinite. \n",
    "\n",
    "The function f is defined as \n",
    "\n",
    "\\begin{align*}\n",
    "\tf(x) = \\tfrac{1}{2}x^T Q x + q^Tx\n",
    "\\end{align*}\n",
    "\n",
    "and has the Hessian matrix  \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\nabla^2f(x) = Q\n",
    "\\end{align*}\n",
    "\n",
    "where it is given that $Q\\in\\mathbb{S}_{++}^{n}$. This set ${S}_{++}^{n}$ denotes that\n",
    "\n",
    "    \n",
    "\\begin{align*}\n",
    "\t{S}_{++}^{n} = \\{A \\in \\mathbb{S}^{n} | A \\prec 0 \\}\n",
    "\\end{align*}\n",
    "\n",
    "Since positive definit is a stronger notation than positive semi definit, $f$ is convex by the second order condition for convexity.\n",
    "\n",
    "---\n",
    "\n",
    "The epigraf of $\\iota_{S}$ is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "\tepi\\iota_{S} = \\{(x,r)|\\iota_{S}(x) \\leq r\\} = \\{(x,r)|x \\in S, r \\geq 0\\}\n",
    "\\end{align*}\n",
    "\n",
    "Define two points $(x_1,r_1)$ and $(x_2,r_2)$ where $x_1,x_2 \\in S$ and $r_1,r_2 \\geq 0$, i.e. both are part of the epigraf of $\\iota_{S}$, and $\\theta$ such that $0 \\leq \\theta \\leq 1$. The convexity definition for sets gives that the epigraf of $\\iota_{S}$ is convex if and only if:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\theta (x_1,r_1) + (1-\\theta)(x_2,r_2) \\in epi\\iota_{S}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\theta (x_1,r_1) + (1-\\theta)(x_2,r_2) = (/theta x_1 + (1-\\theta)x_2, \\theta r_1 + (1-\\theta)r_2)\n",
    "\\end{align*}\n",
    "\n",
    "Since\n",
    "\n",
    "\\begin{align*}\n",
    "\t/theta x_1 + (1-\\theta)x_2 \\in S \\text{ as S is convex and } \\theta r_1 + (1-\\theta)r_2) \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "We can conclude that\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\theta (x_1,r_1) + (1-\\theta)(x_2,r_2) \\in epi\\iota_{S}\n",
    "\\end{align*}\n",
    "\n",
    "holds and $epi\\iota_{S}$ is convex which in turn proves that $\\iota_{S}$ is convex.\n",
    "\n",
    "--- \n",
    "\n",
    "The constraint qualification (CQ) is defined as the condition\n",
    "\n",
    "\\begin{align*}\n",
    "    relint \\  dom(f o L) \\cap relint \\  dom g \\ne \\emptyset.\n",
    "\\end{align*}\n",
    "\n",
    "Where we have $L$ as the identity matrix, and the funtion $g$ is $\\iota_{S}$. From the instructions we are allowed to assume that relint $\\relint S \\neq \\emptyset$. The (effective) domain of a general function $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$ is the set \n",
    "\n",
    "\\begin{align*}\n",
    "     domf = \\{x \\in\\mathbb{R}^{n} : f(x) < \\infty\\}\n",
    "\\end{align*}\n",
    "\n",
    "We can thus conclude that $\\iota_{S}$ is proper. Since the set S is defined for $x\\in\\mathbb{R}^{n}$ the relint domain of $\\iota_{S}$ must overlap with the relint domain of f, which is defined for all $x\\in\\mathbb{R}^{n}$. The constraint qualification therefore holds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 2:\n",
    "\n",
    "Compute the conjugate functions $f^\\ast$ and $\\iota_{S}^\\ast$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "The conjugate function of $f:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}\\cup\\{\\infty\\}$ is defined as \n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(x) := \\sup_{x}(s^Tx-f(x)).\n",
    "\\end{align*}\n",
    "\n",
    "Starting with coputation of $f^\\ast(s)$ this gives us\n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(s) = \\sup_{x}(s^Tx-\\tfrac{1}{2}x^T Q x - q^Tx).\n",
    "\\end{align*}\n",
    "\n",
    "I.e the conjugate is found by the largest value of this optimization problem. Since Q is positive definite the function is strongly concave, and the maximum therefore exists and is unique. Since the function also is differentiable, Fermat's rule says that $x$ is found by setting the gradient to zero. That is, \n",
    "\n",
    "\\begin{align*}\n",
    "\t0 = s - Qx - q.    \n",
    "\\end{align*}\n",
    "\n",
    "which can be rewritten as \n",
    "\n",
    "\\begin{align*}\n",
    "\tx = Q^{-1}(s-q).    \n",
    "\\end{align*}\n",
    "\n",
    "By inserting the maximizing argument we obtain the largest value, i.e the conjugate. This gives: \n",
    "\n",
    "\\begin{align*}\n",
    "\tf^\\ast(s) = s^TQ^{-1}(s-q) - \\tfrac{1}{2}{(s-q)}^TQ^{-1} Q Q^{-1}(s-q) - q^TQ^{-1}(s-q) \n",
    "    = \\tfrac{1}{2}(s-q)^TQ^{-1}(s-q)\n",
    "\\end{align*}\n",
    "\n",
    "--- \n",
    "\n",
    "Now for computating $\\iota_{S}^\\ast$ we denote \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}^\\ast(s) = \\sup_{x}(s^Tx-\\iota_{S}(x)).\n",
    "\\end{align*}\n",
    "\n",
    "Since $\\iota_{S}(x)$ is $\\infty$ for $x\\in \\mathbb{R}^n \\setminus S$ and $0$ for $x\\in \\mathbb{S}$, and it is $-\\iota_{S}(x)$ in the optimization problem, the maximum value will be found for $x\\in \\mathbb{S}$. The conjugate can therefore be rewritten as\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}^\\ast(s) = \\sup_{x\\in \\mathbb{S}}(s^Tx).\n",
    "\\end{align*}\n",
    "\n",
    "Given the seperability of linear terms we have \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S}^\\ast(s) = \\sup_{x\\in \\mathbb{S}}(s^Tx-\\iota_{S}(x)) =\n",
    "    \\sup_{(x_1,\\ldots, x_n)\\in \\mathbb{S}} (\\sum_{i=1}^{n} (s_i x_i - \\iota_{S_i}(x_i)))  =\n",
    "    \\sum_{i=1}^{n} \\sup_{x_i\\in \\mathbb{R}}(s_i x_i - \\iota_{S_i}(x_i))     = \\sum_{i=1}^{n} \\iota_{S_i}^\\ast(s_i)\n",
    "\\end{align*}\n",
    "\n",
    "Since the function is seberale we can look at $\\iota_{S_i}(x_i)$. For $\\iota_{S_i}(x_i)$ we have the subgradient \n",
    "\n",
    "\\begin{align*}\n",
    "\t\\partial \\iota_{S_i}(x_i) =\n",
    "\t\\begin{cases}\n",
    "        (-\\infty, 0]\t& \\text{if }x_i = a_i, \\\\\n",
    "        0 \t\t \t\t& \\text{if }a_i<x_i<b_i, \\\\\n",
    "        [0, \\infty) \t& \\text{if }x_i = b_i, \\\\\n",
    "        \\emptyset\t\t& \\text{otherwise }\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "And we can thus consider three cases to determine $\\iota_{S_i}^\\ast$. $s_i\\in \\mathbb(-\\infty, 0]$, $s_i = 0$ and $s_i\\in \\mathbb[0,\\infty)$. Suppose with $s_i\\in \\mathbb(-\\infty, 0]$, then $s_i\\in \\mathbb \\partial \\iota_{S_i}(x_i)$ implies that $x_i = a_i$. Fenchel Youngs equalite then gives\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S_i}^\\ast(s_i) = s_i * a  - 0 = s_ia_i.\n",
    "\\end{align*}\n",
    "\n",
    "Now suppose $s_i = 0$. $s\\in \\mathbb \\partial \\iota_{S_i}(x_i)$ implies that $x\\in \\mathbb[a,b]$. Fenchel Youngs equality then gives\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S_i}^\\ast(s_i) = 0*x - 0 = 0 = s_ia_i = s_ib_i.\n",
    "\\end{align*}\n",
    "\n",
    "For the last case suppose $s_i\\in \\mathbb[0,\\infty)$. Then $s\\in \\mathbb \\partial \\iota_{S_i}(x_i)$ implies that $x_i = b_i$ and Fenchel Youngs equalite gives\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S_i}^\\ast(s_i) = s_i * b_i  - 0 = s_ib_i.\n",
    "\\end{align*}\n",
    "\n",
    "In conclusion the conjugate can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\iota_{S_i}^\\ast(s_i) =\n",
    "\t\\begin{cases}\n",
    "\t\ts_i * a_i\t\t& \\text{if }s_i \\in \\mathbb(-\\infty, 0], \\\\\n",
    "        0 \t\t \t\t& \\text{if }s_i = 0, \\\\\n",
    "        s_i * b_i\t\t& \\text{if }s_i \\in \\mathbb[0, \\infty), \\\\\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "We now have $\\iota_{S_i}^\\ast(s_i)$ and since we know that $\\iota_{S}^\\ast(s)$ is separable we have now covered all cases and can conclude that \n",
    "\n",
    "\\begin{align*}\n",
    "    \\iota_{S}^\\ast(s) = \\sum_{i=1}^{n} \\max(a_i s_i, b_i s_i)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 3:\n",
    "\n",
    "Write down a Fenchel dual problem to (2). Show that constraint qualification for the dual problem (CQ-D) holds.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "Consider the primal copmposite optimization problem \n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(Lx) + \\iota_{S}(x)\n",
    "\\end{align*}\n",
    "\n",
    "where $f$ and $\\iota_{S}(x)$ are closed convex functions and with $L$ being the identity matrix. Since CQ holds (see task 1) the corresponding dual problem can be written as: \n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{\\mu \\in \\mathbb{R}^n}{\\text{minimize}}\\; f^*(-\\mu) + \\iota^*_{S}(\\mu) = \\tfrac{1}{2}(\\mu+q)^TQ^{-1}(\\mu+q) + \\sum_{i=1}^{n} \\max(a_i\\mu_i, b_i\\mu_i)\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\begin{cases}\n",
    "\t\t-\\mu \\in \\partial f(x) \\\\\n",
    "\t\t\\mu \\in \\partial \\iota_{S}(x)\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The constraint qualification for the dual problem is \n",
    "\n",
    "\\begin{align*}\n",
    "    relint \\ dom (f^\\ast \\ o -L^T) \\cap relint \\ dom \\ \\iota_{S}^\\ast \\ne \\emptyset.\n",
    "\\end{align*}\n",
    "\n",
    "This holds since \n",
    "\n",
    "\\begin{align*}\n",
    "    relint \\ dom (f^\\ast o -I) \\cap relint \\ dom (\\iota_{S}^\\ast) \n",
    "    \\\\= relint \\ dom f^\\ast \\cap relint \\ dom (\\iota_{S}^\\ast) \n",
    "    \\\\= relint \\ dom f^\\ast \\cap relint R^n\n",
    "    \\\\= relint \\ dom f^\\ast \\cap R^n\n",
    "    \\\\= relint \\ dom f^\\ast \n",
    "    \\\\ \\ne 0. \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 4:\n",
    "\n",
    "Show that $f$ and $f^*$ are $\\beta$-, and $\\beta^*$-smooth, respectively. Find expressions for the smallest such parameters $\\beta$ and $\\beta^*$.\n",
    "\n",
    "_Hint:_ Later when calculating the smoothness parameters in Pyhton, make sure to read the documentation carefully so that you use the correct function.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "As previously shown $f$ is convex, and by defintition a conjugate is always convex since it is the epigraph intersection of convex halfspaces. Also note that both $f$ and $f^*$ are differentiable for all $x$. The second-order condition for smoothness for convex functions states that a function is beta smooth and convex if and only if: \n",
    "\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f(x) \\preceq \\beta I\n",
    "\\end{align*}\n",
    "\n",
    "For $f$ and $f^*$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f(x) = Qx + q \\implies \\nabla^2 f(x) = Q\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f^* (x) = Q^{-1} (s-q) \\implies \\nabla^2f^* (x) = Q^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "Since $Q$ is positive definite, both $Q$ and $Q^{-1} \\succ 0$, and thus the lower bound holds. $Q$ is defined, and thus the upperbound also holds. More specifically the smallest parameters for $\\beta$ respectively $\\beta^*$ are:\n",
    "\n",
    "\\begin{align*} \n",
    "    \\beta = \\lambda_{max}\n",
    "\\end{align*}\n",
    "\\begin{align*} \n",
    "    \\beta^* = \\lambda^*_{max}\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_{max}$ is the biggest eigenvalue for $Q$ and $\\lambda^*_{max}$ is the biggest eigenvalue for $Q^{-1}$. In conclusion we can write: \n",
    "\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f(x) = Q \\preceq \\beta I\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "   0 \\preceq \\nabla^2f^* (x) = Q^{-1} \\preceq \\beta^* I\n",
    "\\end{align*}\n",
    "\n",
    "and so by the second-order condition for smoothness for convex functions it is proven that $f$ and $f^*$ are $\\beta$- respectively $\\beta^*$-smooth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "$\\DeclareMathOperator*{\\argmin}{argmin}$\n",
    "\n",
    "---\n",
    "### Task 5:\n",
    "\n",
    "Compute $\\nabla f$, $\\nabla f^\\ast$, $\\prox_{\\gamma\\iota_{S}}$ and $\\prox_{\\gamma\\iota_{S}^\\ast}$. The final expressions are not allowed to be given implicitly via optimization problems. E.g., projection formulas must be solved explicitly.\n",
    "\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "From previous tasks we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    f(x) =\\tfrac{1}{2}x^T Q x + q^Tx \\implies \\nabla f(x) = Qx + q\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    f^*(x) = \\tfrac{1}{2}(s-q)^TQ^{-1}(s-q) \\implies \\nabla f^* (x) = Q^{-1} (s-q)\n",
    "\\end{align*}\n",
    "\n",
    "For $\\prox_{\\gamma\\iota_{S}}$ and $\\prox_{\\gamma\\iota_{S}^\\ast}$ Fermats rule gives:\n",
    "\n",
    "\\begin{align*}\n",
    "    x = \\prox_{\\gamma\\iota_{S}}(z)\n",
    "\\end{align*}\n",
    "\n",
    "if and only if\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z-x) \\in \\partial \\iota_{S}(x)\n",
    "\\end{align*}\n",
    "\n",
    "__For $\\iota_{S}(x)$ we have:__\n",
    "\n",
    "Note that $\\iota_{S}(x)=\\sum_{i=1}^{n}\\iota_{S}(x_i)$. As the function is seperable this means:\n",
    "    \n",
    "\\begin{align*}\n",
    "    \\prox_{\\gamma\\iota_{S}}(z)=(\\prox_{\\gamma\\iota_{S1}}(z_{1}),...,\\prox_{\\gamma\\iota_{Sn}}(z_{n}))\n",
    "\\end{align*}\n",
    "\n",
    "The subdiferential for $\\iota_{Si}(x)$ is calculated to:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\partial \\iota_{Si}(x) =\n",
    "\t\\begin{cases}\n",
    "        (-\\infty, 0]\t& \\text{if }x_i = a_i, \\\\\n",
    "        0 \t\t \t\t& \\text{if }a_i<x_i<b_i, \\\\\n",
    "        [0, \\infty) \t& \\text{if }x_i = b_i, \\\\\n",
    "        \\emptyset\t\t& \\text{otherwise},\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "__Fermats rule gives:__\n",
    "\n",
    "For $x_i=a_i$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-x_i) \\in (-\\infty, 0] \\implies z_i \\in (-\\infty, a_i]\n",
    "\\end{align*}\n",
    "\n",
    "For $x_i \\in (a_i,b_i)$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-x_i) \\in \\{0\\} \\implies z_i=x_i\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For $x_i=b_i$ we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-x_i) \\in [0, \\infty) \\implies z_i \\in [b_i, \\infty)\n",
    "\\end{align*}\n",
    "\n",
    "This gives:\n",
    "\n",
    "\\begin{align*}\n",
    "\t\\prox_{\\gamma\\iota_{Si}}(z_i) =\n",
    "\t\\begin{cases}\n",
    "        a_i \t\t \t\t& \\text{if }z_i \\leq a_i, \\\\\n",
    "        z_i \t\t\t\t& \\text{if }z_i \\in (a_i,b_i) \\\\\n",
    "        b_i \t\t\t\t& \\text{if }z_i \\geq b_i, \\\\\n",
    "\t\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "__For $\\iota^*_{S}(x)$ we have:__\n",
    "\n",
    "As $\\iota^*_{S}(s) =\\sum_{i=1}^{n} \\max(a_i s_i, b_i s_i)$ is seperable (see task 2) we know that:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\prox_{\\gamma\\iota^*_{S}}(z) = (\\prox_{\\gamma\\iota^*_{S1}}(z_{1}),...,\\prox_{\\gamma\\iota^*_{Sn}}(z_{n})).\n",
    "\\end{align*}\n",
    "\n",
    "That is the proxy decomposes into $n$ individual proxes, which gives us n independent optimization problems. \n",
    "\n",
    "The subdiferential for $\\iota^*_{Si}(s_{i})$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\partial \\iota^*_{Si}(s_{i}) =\n",
    "    \\begin{cases}\n",
    "        a_i & \\text{if } s_i<0\\\\\n",
    "        [a_i,b_i] & \\text{if } s_i=0\\\\\n",
    "        b_i & \\text{if } s_i>0\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Fermats rule gives:\n",
    "\n",
    "For $s_i<0$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-s_i) \\in \\{a_i\\} \\implies s_i=z_i-\\gamma a_i\n",
    "\\end{align*}\n",
    "\n",
    "For $s_i=0$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-s_i) \\in [a_i,b_i] \\implies z_i \\in [\\gamma a_i, \\gamma b_i]\n",
    "\\end{align*}\n",
    "\n",
    "For s_i>0:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma^{-1}(z_i-s_i) \\in \\{b_i\\} \\implies s_i=z_i-\\gamma b_i\n",
    "\\end{align*}\n",
    "\n",
    "This gives:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\prox_{\\gamma\\iota^*_{Si}}(z_{i})=\n",
    "    \\begin{cases}\n",
    "        z_i-\\gamma a_i & \\text{if } z_i<\\gamma a_i\\\\\n",
    "        0 & \\text{if } z_i \\in [\\gamma a_i, \\gamma b_i]\\\\\n",
    "        z_i-\\gamma b_i & \\text{if } z_i > \\gamma b_i\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 6:\n",
    "\n",
    "Based on your results above, write explicitly out the proximal gradient update rule (4) for both the primal and the dual problem. Use $x$ as the primal variable and $\\mu$ as the dual variable.\n",
    "\n",
    "_Attention/hint:_ Keep track of your minus signs.\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "The primal problems update rule is given by:\n",
    "\\begin{align*}\n",
    "\tx^{k+1} = \\prox_{\\gamma \\iota_{S}}\\left(x^k - \\gamma \\nabla f(x^k)\\right) = \n",
    "    \\prox_{\\gamma \\iota_{S}}\\left(x^k - \\gamma (Qx^k - q)\\right)\\\\ \n",
    "    \\begin{cases}\n",
    "        v^k = x^k +\\gamma (Qx^k-q)\\\\\n",
    "        (\\mu^{k+1})_i =\n",
    "            \\begin{cases}\n",
    "                b_i \t\t\t\t& \\text{if }v^k_i \\geq b_i, \\\\\n",
    "                v^k_i \t\t\t\t& \\text{if }v^k_i \\in (a_i,b_i) \\\\\n",
    "                a_i \t\t \t\t& \\text{if }v^k_i \\leq a_i, \\\\\n",
    "            \\end{cases}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The dual problems uppdate rule is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu^{k+1} = \\prox_{\\gamma \\iota^*_{S}}\\left(\\mu^k - \\gamma \\nabla (f^* o -I)(\\mu^k)\\right),\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "     \\nabla (f^* o -I)(\\mu^k) = - \\nabla f^*(-\\mu^k) = Q^{-1}(\\mu^k+q).\n",
    "\\end{align*}\n",
    "\n",
    "We can therefor derive the dual problems update rule from:    \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu^{k+1} = \\prox_{\\gamma \\iota^*_{S}} (\\mu^k - \\gamma Q^{-1}(\\mu^k+q)). \n",
    "\\end{align*}\n",
    "  \n",
    "  \n",
    "Recall that the prox decomposes into $n$ individual proxes (see task 5). Thus, the proximal gradient method step for the dual problem becomes:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\begin{cases}\n",
    "        u^k = \\mu^k - \\gamma Q^{-1}(\\mu^k+q)\\\\\n",
    "        (\\mu^{k+1})_i =\n",
    "        \\begin{cases}\n",
    "            b_i \t\t\t\t& \\text{if }u^k_i \\geq b_i, \\\\\n",
    "            u^k_i\t\t\t& \\text{if }u^k_i \\in (a_i,b_i) \\\\\n",
    "            a_i \t\t \t\t& \\text{if }u^k_i \\leq a_i, \\\\\n",
    "        \\end{cases}\n",
    "    \\end{cases}  \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 7:\n",
    "\n",
    "Suppose that $\\mu^\\star\\in\\R^{n}$ is an optimal solution to the dual problem you found in Task 3. Given $\\mu^\\star$, and starting from the optimality condition for the dual problem (given by Fermat's rule), recover an optimal point $x^{\\star}\\in\\R^{n}$ to the primal problem (2), and show that this $x^{\\star}$ is in fact an optimal solution to the primal problem (2).\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "The dual optimality condition given by Fermat's rule says that $\\mu^\\star$ minimizes the dual if and only if\n",
    "\n",
    "\\begin{align*}\n",
    "    0 \\in \\partial f^*(\\mu^*) - L\\partial g^*(-L^T\\mu^*)\n",
    "\\end{align*}\n",
    "\n",
    "This is equivelent to writing \n",
    "\n",
    "\\begin{align*}\n",
    "    Lx^* \\in \\partial f^*(\\mu^*)\n",
    "    \\\\ x^* \\in \\partial g^*(-L^T\\mu^*).\n",
    "\\end{align*}\n",
    "\n",
    "Using the subdifferential on the second condition we have \n",
    "\n",
    "\\begin{align*}\n",
    "    x^* \\in \\partial g^*(-L^T\\mu^*) \\ \\ \\Longleftrightarrow \\ -L^T\\mu^* \\in \\partial g(x^*)\n",
    "\\end{align*}\n",
    "\n",
    "and the conditions can then be rewritten as\n",
    "\n",
    "\\begin{align*}\n",
    "    Lx^* \\in \\partial f^*(\\mu^*)\n",
    "    \\\\ -L^T\\mu^* \\in \\partial g(x^*).\n",
    "\\end{align*}\n",
    "\n",
    "Now repeting the procedure but with the first condition. That is taking the subdifferential of the first condition, we get \n",
    "\n",
    "\\begin{align*}\n",
    "    Lx^* \\in \\partial f^*(\\mu^*) \\ \\ \\Longleftrightarrow \\ \\mu \\in \\partial f(Lx^*)\n",
    "\\end{align*}\n",
    "\n",
    "and thus the conditions can once again be rewritten as \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu^* \\in \\partial f(Lx^*)\n",
    "    \\\\ -L^T\\mu^* \\in \\partial g(x^*).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "These last conditions is equivelent to writing \n",
    "\n",
    "\\begin{align*}\n",
    "    0 \\in L^T \\partial f(Lx^*) + \\partial g(x^*)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The primal optimality condition says that for functions $f:\\mathbb{R}^{m}\\rightarrow\\mathbb{R}$, $g:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}$, $L \\in \\mathbb{R}^{m}x\\mathbb{R}^{n}$, with $f$ and $g$ closed convex and assuming CQ holds, then \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\underset{x \\in \\mathbb{R}^{n}}{\\text{minimize}}\\; f(Lx) + g(x)\n",
    "\\end{align*}\n",
    "\n",
    "is solved by $x^* \\in R^n$ if and only if $x^*$ satisfies \n",
    "\n",
    "\\begin{align*}\n",
    "    0 \\in L^T \\partial f(Lx^*) + \\partial g(x^*)\n",
    "\\end{align*}. \n",
    "\n",
    "\n",
    "Since this is both the form of our primal problem (with $g(x)$ = $\\iota_{S}(x)$ and $L$ being the identity matrix) and the condition that is given from the $x^*$ that has been derived from $u^*$ it is therefore proven that the recovered optimal point $x^*$ is in fact an optimal solution to the primal problem. \n",
    "\n",
    "More explicitly from equation 5 we have\n",
    "\n",
    "\\begin{align*}\n",
    "    Lx^* \\in \\partial f^*(\\mu^*).\n",
    "\\end{align*}\n",
    "\n",
    "In our case this can be rewritten as\n",
    "\n",
    "\\begin{align*}\n",
    "    x^* \\in \\{\\nabla f^*(\\mu^*)\\}\n",
    "\\end{align*}\n",
    "\n",
    "since $f^*$ is differentiable (the subgradient of the conjugate is the same as the gradient of the conjugate) and since $L$ is the identity matrix. Furthermore, since $x$ is a singleton the optimal solution to the primal can be calculated by\n",
    "\n",
    "\\begin{align*}\n",
    "    x^* = \\nabla f^*(\\mu^*)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "--- \n",
    "### Task 8:\n",
    "\n",
    "Use your results above to fill in the functions below.\n",
    "\n",
    "__Solution:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    quad(x,Q,q) computes the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    ans = (1/2)*(x@Q@x+q@x)\n",
    "    return ans\n",
    "\n",
    "def quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    quadconj(mu,Q,q) computes the conjugate function of the \n",
    "    quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    Qinv = np.linalg.inv(Q)\n",
    "    ans = (1/2) * (np.subtract(mu,q).T@Qinv@(mu-q))\n",
    "    return ans\n",
    "\n",
    "def box(x,a,b):\n",
    "    \"\"\"\n",
    "    box(x,a,b) computes the indicator function of the box contraint\n",
    "    [a,b]\n",
    "    \n",
    "    :param x: the variable of the indicator function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: 0 if x is in [a,b] and infinity otherwise\n",
    "    \"\"\"\n",
    "    if np.all(a <= x) and np.all(x <= b):\n",
    "        return 0\n",
    "    else: \n",
    "        return np.Inf\n",
    "\n",
    "def boxconj(mu,a,b):\n",
    "    \"\"\"\n",
    "    boxconj(mu,a,b) computes the conjugate function of the indicator function \n",
    "    of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: conjugate of the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    ans = 0\n",
    "    l = len(mu)\n",
    "    for i in range (0, l):\n",
    "        mua = mu[i]*a[i]\n",
    "        mub = mu[i]*b[i]\n",
    "        if(mua>mub):\n",
    "            ans += mua\n",
    "        else:\n",
    "            ans += mub\n",
    "    return ans\n",
    "\n",
    "def grad_quad(x,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quad(x,Q,q) computes the gradient of the quadratic function (1/2)x'Qx + q'x\n",
    "    \n",
    "    :param x: the variable of the quadratic function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of (1/2)x'Qx + q'x\n",
    "    \"\"\"\n",
    "    ans = Q@x+q\n",
    "    return ans\n",
    "\n",
    "def grad_quadconj(mu,Q,q):\n",
    "    \"\"\"\n",
    "    grad_quadconj(mu,Q,q) computes the gradient of the conjugate function of the \n",
    "    the quadratic function (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the conjugate function\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :return: gradient of the conjugate of (1/2)x'Qx + q'x, evaluated at mu\n",
    "    \"\"\"\n",
    "    Qinv = np.linalg.inv(Q)\n",
    "    ans = Qinv(mu-q)\n",
    "    return ans\n",
    "\n",
    "def prox_box(x,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(x,a,b,gamma) computes proximal operator of the indicator function \n",
    "    of the box contraint [a,b], evaluated at x\n",
    "    \n",
    "    :param x: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the indicator function of the \n",
    "    box contraint [a,b], evaluated at x\n",
    "    \"\"\"\n",
    "    proxBox = []\n",
    "    l = len(x)\n",
    "    for i in range(0, l-1):\n",
    "        if(x[i] >= b[i]):\n",
    "            proxBox.append(b[i])\n",
    "        elif(x[i] <= a[i]):\n",
    "            proxBox.append(a[i])\n",
    "        else:\n",
    "            proxBox.append(x[i])\n",
    "    return proxBox\n",
    "        \n",
    "\n",
    "def prox_boxconj(mu,a,b,gamma):\n",
    "    \"\"\"\n",
    "    prox_box(mu,a,b,gamma) computes proximal operator of the conjugate function of \n",
    "    the indicator function of the box contraint [a,b], evaluated at mu\n",
    "    \n",
    "    :param mu: the variable of the poximal operator\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :param gamma: the step-size parameter\n",
    "    :return: proximal operator of the conjugate function of the indicator function of the \n",
    "    box contraint [a,b], evaluated at mu\n",
    "    \"\"\"\n",
    "    proxCon = []\n",
    "    l = len(mu)\n",
    "    for i in range (0, l):\n",
    "        if(mu[i] < gamma*a[i]):\n",
    "            proxCon.append(mu[i]-gamma*a[i])\n",
    "        elif(mu[i]>gamma*b[i]):\n",
    "             proxCon.append(mu[i]-gamma*b[i])\n",
    "        else:\n",
    "             proxCon.append(0)\n",
    "\n",
    "    return proxCon\n",
    "\n",
    "def dual_to_primal(mu,Q,q,a,b):\n",
    "    \"\"\"\n",
    "    dual_to_primal(mu,Q,q,a,b) computes the solution x* to the primal problem \n",
    "    given a solution mu* to the dual problem.\n",
    "    \n",
    "    :param mu: the dual variable\n",
    "    :param Q: the matrix in the quadratic function that corresponds to the quadratic form\n",
    "    :param q: the vector in the quadratic function that corresponds to the linear part\n",
    "    :param a: the left vector defining the box contraint\n",
    "    :param b: the right vector defining the box contraint\n",
    "    :return: the extracted primal variable\n",
    "    \"\"\"\n",
    "    x = grad_quadconj(mu,Q,q)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 9:\n",
    "\n",
    "Below is a function for generating $Q$, $q$, $a$, and $b$ that define the quadratic function $f$ and the box constraint set $S$. Use Task 8 to solve the primal problem using the proximal gradient method.\n",
    "\n",
    "__a)__ What seems to be the best choice of $\\gamma$? \n",
    "\n",
    "__b)__ Does the upper bound $\\gamma < \\frac{2}{\\beta}$ seem reasonable?\n",
    "\n",
    "\n",
    "Test different initial points for the algorithm:\n",
    "\n",
    "__c)__ Does this affect the point the algorithm converges to? \n",
    "\n",
    "__d)__ Reason about why/why not it affects the final point. _Hint:_ Look at the objective function in (2).\n",
    "\n",
    "__e)__ Does your final point $x^{\\text{final}}$ satisfy the constraint $x^{\\text{final}} \\in S$?\n",
    "\n",
    "__f)__ What about the iterates, do they always satisfy the constraint, $x^k \\in S$? Why/why not?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivar\\AppData\\Local\\Temp\\ipykernel_31452\\4235671528.py:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if(xn == xk):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     gradX \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_quad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     xn \u001b[38;5;241m=\u001b[39m prox_box(x, a, b, gamma)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(xn \u001b[38;5;241m==\u001b[39m xk):\n",
      "Input \u001b[1;32mIn [73]\u001b[0m, in \u001b[0;36mgrad_quad\u001b[1;34m(x, Q, q)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_quad\u001b[39m(x,Q,q):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    grad_quad(x,Q,q) computes the gradient of the quadratic function (1/2)x'Qx + q'x\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    :return: gradient of (1/2)x'Qx + q'x\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     ans \u001b[38;5;241m=\u001b[39m Q\u001b[38;5;129m@x\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241m+\u001b[39mq\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ans\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "def problem_data():\n",
    "    \"\"\"\n",
    "    problem_data() generates the problem data variables Q, q, a and b\n",
    "    \n",
    "    :return: (Q,q,a,b)\n",
    "    \"\"\"\n",
    "    rs = np.random.RandomState(np.random.MT19937(np.random.SeedSequence(1)))\n",
    "    n = 20\n",
    "    Q = rs.randn(n,n)\n",
    "    Q = Q.T@Q\n",
    "    q = rs.randn(n)\n",
    "    a = -rs.rand(n)\n",
    "    b = rs.rand(n)\n",
    "    return Q, q, a, b\n",
    "\n",
    "(Q,q,a,b) = problem_data()\n",
    "\n",
    "# Write your solution here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator{\\prox}{prox}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\Sym}{\\mathbb{S}}$\n",
    "$\\newcommand{\\norm}[1]{\\lVert{#1}\\rVert}$\n",
    "$\\DeclareMathOperator{\\sign}{sign}$\n",
    "$\\DeclareMathOperator{\\diag}{diag}$\n",
    "$\\DeclareMathOperator{\\relint}{relint}$\n",
    "$\\DeclareMathOperator{\\dom}{dom}$\n",
    "---\n",
    "### Task 10:\n",
    "\n",
    "Solve the dual problem. \n",
    "\n",
    "__a)__ Similar to the previous task, find/verify the upper bound on the step-size and find a good step-size choice.\n",
    "\n",
    "Let $x^{\\text{final}}$ be the final points from Task 9 and $\\mu^{\\text{final}}$ the final point for the dual problem. Let $\\hat{x}^{\\text{final}}$ final primal points extracted from the final dual point $\\mu^{\\text{final}}$ using the expression from Task 7:\n",
    "\n",
    "__b)__ Are $x^{\\text{final}}$ and $\\hat{x}^{\\text{final}}$ the same?\n",
    "\n",
    "__c)__ Is $\\hat{x}^{\\text{final}}$ in the box $S$?\n",
    "\n",
    "__d)__ Let $\\mu^k$ be the iterates of the dual method, using the expression from Task 7, extract the primal iterates $\\hat{x}^k$ from $\\mu^k$. Does $\\hat{x}^k$ always satisfy the constraint $\\hat{x}^k \\in S$?\n",
    "\n",
    "Also: \n",
    "\n",
    "__e)__ How do the function values $f\\left(\\hat{x}^k\\right)$ develop over the iterations?\n",
    "\n",
    "__f)__ What about $f\\left(\\hat{x}^k\\right)+\\iota_{S}\\left(\\hat{x}^k\\right)$?\n",
    "\n",
    "__Solution:__ \n",
    "\n",
    "_Fill in your solution here!_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
